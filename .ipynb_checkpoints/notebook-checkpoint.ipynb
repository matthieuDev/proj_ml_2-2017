{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.sparse import rand\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Moi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this only once to instal the stopwords on your computer.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "x = np.load('embeddings.npy')\n",
    "y = pickle.load(open('cooc.pkl', 'rb'))\n",
    "z = pickle.load(open('vocab.pkl', 'rb'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "tokenizer =  TweetTokenizer()\n",
    "stop = set(stopwords.words('english')+['<user>','<url>',',','-','.','[',']',':','&'])\n",
    "stemmer = SnowballStemmer(\"english\");\n",
    "mojitable = pickle.load(open('emojtable.pkl', 'rb'))\n",
    "\n",
    "prog = re.compile(\"\\s((?:[^\\s][^\\w\\n]+)(?:[^\\s](?:[^\\w\\n]+|$))+)\")\n",
    "\n",
    "def replaceMoji(str):\n",
    "    splitted = prog.split(str)\n",
    "    matched = prog.finditer(str)\n",
    "    res = \"\"\n",
    "    for s,g in zip(splitted,matched):\n",
    "          res += s + mojitable[str(g.group(1))]\n",
    "    return res;\n",
    "def preparOneTweets(tweet):\n",
    "    return \" \".join([stemmer.stem(replaceMoji(i)) for i in tokenizer.tokenize(tweet.lower()) if i not in stop])\n",
    "def preprocessFile(file):\n",
    "    lines = []\n",
    "    with open(file,'rb') as f :\n",
    "        lines = map(preparOneTweets,f.readlines())\n",
    "        f.close()\n",
    "    with open(\"cleaned\"+file,\"w\") as f:\n",
    "        f.write(\"\\n\".join(list(lines)))\n",
    "        f.close()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other idea for preprocessing\n",
    "\n",
    "Many emoticons have been left in the **test** data. We could had a preprocessing step that replace identified emoji pattern by emoji character, so we do not mistake them with ponctuation.\n",
    " * **: \\*** Used for \"kiss\" 😘 (and only this) in test set. This one is trickyas it contain a whitespace\n",
    " * ** <3 ** Used for ❤ in test set. \n",
    " * ** )** or **) )** or **)) ** Used for ☺ or 😁 for those one we may want to check parentheses balancing.\n",
    " * **( { } )** This thing seem to have a (positive) meaning.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cooc(sourcePkl,pos,neg,target):\n",
    "    with open(sourcePkl+'.pkl', 'rb') as f:\n",
    "        vocab = pickle.load(f)\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    data, row, col = [], [], []\n",
    "    counter = 1\n",
    "    for fn in [pos+'.txt', neg+'.txt']:\n",
    "        with open(fn) as f:\n",
    "            for line in f:\n",
    "                tokens = [vocab.get(t, -1) for t in line.strip().split()]\n",
    "                tokens = [t for t in tokens if t >= 0]\n",
    "                for t in tokens:\n",
    "                    for t2 in tokens:\n",
    "                        data.append(1)\n",
    "                        row.append(t)\n",
    "                        col.append(t2)\n",
    "\n",
    "                if counter % 10000 == 0:\n",
    "                    print(counter)\n",
    "                counter += 1\n",
    "    cooc = coo_matrix((data, (row, col)))\n",
    "    print(\"summing duplicates (this can take a while)\")\n",
    "    cooc.sum_duplicates()\n",
    "    \n",
    "    with open('cooc.pkl', 'wb') as f:\n",
    "        pickle.dump(cooc, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pickleVocab(source,target):\n",
    "    vocab = dict()\n",
    "    with open(source+'.txt') as f:\n",
    "        for idx, line in enumerate(f):\n",
    "            vocab[line.strip()] = idx\n",
    "\n",
    "    with open(target+'.pkl', 'wb') as f:\n",
    "        pickle.dump(vocab, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning of the neg and pos file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-b32661867356>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocessFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_pos.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mpreprocessFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'train_neg.txt'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bf9ba452389e>\u001b[0m in \u001b[0;36mpreprocessFile\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cleaned\"\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"w\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlines\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bf9ba452389e>\u001b[0m in \u001b[0;36mpreparOneTweets\u001b[1;34m(tweet)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreparOneTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplaceMoji\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocessFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bf9ba452389e>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreparOneTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;34m\" \"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstemmer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplaceMoji\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocessFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-bf9ba452389e>\u001b[0m in \u001b[0;36mreplaceMoji\u001b[1;34m(str)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplitted\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmatched\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m           \u001b[0mres\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmojitable\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreparOneTweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object is not callable"
     ]
    }
   ],
   "source": [
    "preprocessFile('train_pos.txt')\n",
    "preprocessFile('train_neg.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Befor rerunning anything bellow, be sure that you have generated the vocab cut with sh scripts\n",
    "\n",
    "Change the variable bellow if the rest of this script should use other file, for exemple the original ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negFile = 'cleanedtrain_neg'\n",
    "posFile = 'cleanedtrain_pos'\n",
    "vocabFile = 'cleanedVocab'\n",
    "coocFile = 'cleanedCooc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pickleVocab(vocabFile,vocabFile)\n",
    "cooc(vocabFile,posFile,negFile,coocFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets = []\n",
    "for fn in ['train_pos.txt', 'train_neg.txt']:\n",
    "        with open(fn) as f:\n",
    "            tweets =  tweets + f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'just woke up , finna go to church\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape\n",
    "tweets[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.56403988 -0.33822548  0.11260476  0.05975893 -0.39520949 -0.2375117\n",
      " -0.12390807 -0.16872797 -0.02044019  0.32865794 -0.64644995 -0.08901839\n",
      "  0.26161983 -0.1631089   0.08588484  0.57699542 -0.52317936 -0.37408539\n",
      " -0.13934747 -0.61597906]\n"
     ]
    }
   ],
   "source": [
    "print(x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thibault = list(zip(y.row, y.col, y.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_map = {v: k for k, v in z.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ranksMax = np.argpartition(x,-10,0)[-10:,:]\n",
    "ranksMin = np.argpartition(x,10,0)[:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "rotation,vmware,xoxoxox,sae,upcoming,kbye,revealing,inexpensive,01910,#askretwittings\n",
      "needy,crisco,marrikas,bismillah,hecka,shortwave,diode,510,meryl,seek\n",
      "1\n",
      "multi-purpose,profession,happybirthday,debuts,a-levels,clifton,task,attractive,hotness,capturing\n",
      "mami,anagallis,d1816,galau,#seeyoushinee,barge,existed,belinda,creator,crust\n",
      "2\n",
      "authority,#yougetmajorpoints,bootcamp,shae,fruity,awsome,sensible,dood,breakdown,childish\n",
      "wann,ottawa,rowland,#askgi,themmm,anywho,opportunities,apprenticeship,zebra,header\n",
      "3\n",
      "kyu,streaming,mbak,garri,adi,simplify,raul,elegance,somemore,desig\n",
      "ellie,sfb-hpdv,xoxoxo,sabtu,collective,hb,pv,asleeep,shutout,blocked\n",
      "4\n",
      "blackout,telescope,)':,#future,muriaticum,twitch,buddhist,rivalry,vips,individuall\n",
      "robin,#earthday,shitting,khmer,cheaper,discount,messaging,lyfe,fren,mfc\n",
      "5\n",
      "ohkay,#hurryup,ernst,x12,tomatoes,objects,manner,nipples,q1,weve\n",
      "applies,binder,consuming,striped,citrus,retard,reflection,bachmann,socrates,picnic\n",
      "6\n",
      "2/2,neverrr,waay,rosehips,srry,handing,pulp,haaay,swirl,routes\n",
      "oww,kidnapping,remark,islamic,originals,revise,grateful,dsi,adoption,silence\n",
      "7\n",
      "rollin,mystique,jolly,unlikely,wives,#onedirection,scold,focuses,wohooo,cld\n",
      "wib,aeg,recording,berserk,#why,ditched,investors,confidence,niggaz,ports\n",
      "8\n",
      "resume,biased,2pac,ensuring,sorrry,guests,tricia,chooses,tipsy,wink\n",
      "visitor,ithink,#kidding,eng,aug,marines,hallmark,rushing,geographic,redhead\n",
      "9\n",
      "boyce,yamaha,honda,nomore,goodmornin,supportive,crawford,ingredient,sealing,licorice\n",
      "blowin,corning,mccartney,passage,magically,1978,backk,vegan,chiropractic,happpyyy\n",
      "10\n",
      "streaming,tlkn,elegantly,discourses,slammed,#thatawkwardmoment,px,#addicted,#waystobeginsex,sprained\n",
      "activated,twitta,sel,craziness,smoked,630,jesse,errors,bekasi,12.04\n",
      "11\n",
      "zeikos,pastoral,cincinnati,bollywood,cracker,haven,tango,chambers,hmp,cheered\n",
      "pst,heartbreaking,pauly,transitions,valentine's,supportive,goldfish,225,ians,130ashbk\n",
      "12\n",
      "memoirs,stewie,promotion,shea,geisha,shaking,49x,earthquakes,briefcase,10ml\n",
      "woken,inquirer,15x31,fantabulous,everquest,bodies,3b,totes,conditioner,safari\n",
      "13\n",
      "afghanistan,pegasus,m1,excitement,breeze,sheridan,dsl,donation,shocking,aja\n",
      "deployment,niall's,audrey,counter,bowie,bts,thot,creek,plasma,touring\n",
      "14\n",
      "elegance,#priceless,owh,motherboard,ro,attached,wetsuit,richest,lakeside,projection\n",
      "printers,cann,schutt,attachment,polarized,veteran,netgear,anorexic,cudi,tutu\n",
      "15\n",
      "farish,average,fotek,1l,murs,#mustfollow,houses,guinness,anzac,angello\n",
      "kee,prog,nottt,sausages,#1dwebstersurfboards,twitt,northland,joyful,16x17,html\n",
      "16\n",
      "crispy,wooh,secretly,snickers,cooo,mtv's,lookout,branded,flippin,tru\n",
      "gears,investigation,unfortunate,janine,#goal,#instagram,sence,strapless,perhaps,somethings\n",
      "17\n",
      "cuhh,breakout,berserk,chaco,unframed,#1dfacts,chucks,pumping,stranded,bumblebee\n",
      "desmoinesregister,judges,tgt,kink,26020,overrated,unemployed,nfb,appearance,dw\n",
      "18\n",
      "elephant,21x30,prostate,abfb,raisin,amooo,virus,rotfl,norway,dada\n",
      "boiled,boardwalk,elph,thot,homerun,danish,icould,essence,rival,2d\n",
      "19\n",
      "bankruptcy,straps,trimmed,splash,boxers,dgaf,bavarian,trapped,eta,partnership\n",
      "hemp,lightbulb,bis,epicenter,geology,#lmfao,nutrition,xoxox,tripoli,approve\n"
     ]
    }
   ],
   "source": [
    "for index,valu in enumerate(zip(ranksMax.T,ranksMin.T)):\n",
    "    print(index)\n",
    "    print(\",\".join(map(lambda x:inv_map[x],valu[0])))\n",
    "    print(\",\".join(map(lambda x:inv_map[x],valu[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.64010273,  3.97650769,  3.9262126 ,  3.83828637,  3.82437642,\n",
       "        4.22734093,  3.87541725,  3.9055985 ,  3.7340799 ,  4.02686927,\n",
       "        3.7251865 ,  3.66711377,  3.92726345,  3.57877701,  3.58190457,\n",
       "        3.70271208,  3.82092118,  3.53813697,  3.37418862,  3.53340649])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(x,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
