{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.tokenize import  TweetTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "tokenizer =  TweetTokenizer()\n",
    "stop = set(stopwords.words('english')+['<user>','<url>',',','-','.','[',']',':','&'])\n",
    "stemmer = SnowballStemmer(\"english\");\n",
    "mojitable = pickle.load(open('emojtable.ðŸ˜ƒ', 'rb'))\n",
    "\n",
    "prog = re.compile(\"(?:\\s|^)((?:[^\\s][^a-zA-Z1-9\\n]+)(?:[^\\s](?:[^a-zA-Z1-9\\n]+|$))+)\")\n",
    "lineHead = re.compile(\"^(\\d+,)\");\n",
    "\n",
    "def replaceMoji(s):\n",
    "    splitted = prog.split(s)\n",
    "    res = \"\"\n",
    "    for i in range(0,int(len(splitted)/2)):\n",
    "        emo = splitted[2*i+1].strip()\n",
    "        parsedEmo = mojitable.get(emo,emo)\n",
    "        if parsedEmo == \"\":\n",
    "            parsedEmo = emo\n",
    "        splitted[2*i+1] = \" \"+parsedEmo+\" \"\n",
    "    \n",
    "    return \"\".join(splitted).strip();\n",
    "def prepareOneLine(line):\n",
    "    try :\n",
    "        tweet = line.decode('utf-8')\n",
    "    except UnicodeDecodeError :\n",
    "        tweet = str(line)[2:-5]\n",
    "        print(\"cant parse \"+tweet)\n",
    "    head = lineHead.search(tweet)\n",
    "    header = \"\"\n",
    "    if head != None :\n",
    "        header = head.group(1)\n",
    "        tweet = tweet[len(header):]\n",
    "    return header+preparOneTweets(tweet)\n",
    "            \n",
    "def preparOneTweets(tweet):    \n",
    "    return  \" \".join([stemmer.stem(replaceMoji(i)) for i in tokenizer.tokenize(tweet) if i not in stop])\n",
    "def preprocessFile(file):\n",
    "    lines = []\n",
    "    with open(file,'rb') as f :\n",
    "        lines = map(prepareOneLine,f.readlines())\n",
    "        f.close()\n",
    "    with open(\"cleaned\"+file,\"w\") as f:\n",
    "        f.write(\"\\n\".join(list(lines)))\n",
    "        f.close()\n",
    "        print(\"done \"+file)\n",
    "\n",
    "preparOneTweets(\"1-3 vs celtic regular season = fuck play playoff\")\n",
    "\n",
    "replaceMoji(\"guess text want us back ( ( (\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "files = [\"train_neg.txt\",\"train_neg_full.txt\",\"train_pos.txt\",\"train_pos_full.txt\",\"test_data.txt\"]\n",
    "for f in files:\n",
    "    preprocessFile(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
