{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.sparse import rand\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "import implementations as imp\n",
    "import proj1_helpers as ph\n",
    "import glove as gl\n",
    "\n",
    "from nltk import pos_tag\n",
    "#nltk.download()\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stem studying: studi\n",
      "Lemmatise studying: studying\n",
      "Lemmatise studying: study\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "lemmatiser = WordNetLemmatizer()\n",
    " \n",
    "print(\"Stem %s: %s\" % (\"studying\", stemmer.stem(\"studying\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\")))\n",
    "print(\"Lemmatise %s: %s\" % (\"studying\", lemmatiser.lemmatize(\"studying\", pos=\"v\")))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'simple', 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "s = \"This is a simple sentence\"\n",
    "tokens = word_tokenize(s) # Generate list of tokens\n",
    "tokens_pos = pos_tag(tokens) \n",
    " \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading cooccurrence matrix\n",
      "6496907 nonzero entries\n",
      "using nmax = 100 , cooc.max() = 207302\n",
      "initializing embeddings\n",
      "epoch 0\n",
      "epoch 1\n",
      "epoch 2\n",
      "epoch 3\n",
      "epoch 4\n",
      "epoch 5\n",
      "epoch 6\n",
      "epoch 7\n",
      "epoch 8\n",
      "epoch 9\n"
     ]
    }
   ],
   "source": [
    "#gl.main(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "glove = np.load('embeddings.npy')\n",
    "y = pickle.load(open('cooc.pkl', 'rb'))\n",
    "vocab = pickle.load(open('vocab.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21161, 20)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "thibault = list(zip(y.row, y.col, y.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inv_vocab = {v: k for k, v in vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_id_tweet (list_tweets) :\n",
    "    ids = [x.split(',')[0] for x in list_tweets]\n",
    "    tweets =  [','.join(x.split(',')[1:]) for x in list_tweets]\n",
    "    return ids , tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1250000\n",
      "2500000\n"
     ]
    }
   ],
   "source": [
    "tweets_train = []\n",
    "smile = []\n",
    "print(len(tweets_train))\n",
    "with open('twitter-datasets/train_neg_full.txt', encoding='UTF-8') as f :\n",
    "    read = f.readlines()\n",
    "    smile = [-1] * len(read)\n",
    "    tweets_train = read\n",
    "print(len(tweets_train))\n",
    "with open('twitter-datasets/train_pos_full.txt', encoding='UTF-8') as f :\n",
    "    read = f.readlines()\n",
    "    smile += [1] * len(read)\n",
    "    tweets_train = np.append(read,tweets_train)\n",
    "    \n",
    "print(len(tweets_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ '<user> i dunno justin read my mention or not . only justin and god knows about that , but i hope you will follow me #believe 15\\n',\n",
       "       \"because your logic is so dumb , i won't even crop out your name or your photo . tsk . <url>\\n\",\n",
       "       '\" <user> just put casper in a box ! \" looved the battle ! #crakkbitch\\n',\n",
       "       \"<user> <user> thanks sir > > don't trip lil mama ... just keep doin ya thang !\\n\",\n",
       "       'visiting my brother tmr is the bestest birthday gift eveerrr ! ! !\\n',\n",
       "       '<user> yay ! ! #lifecompleted . tweet / facebook me to let me know please\\n',\n",
       "       '<user> #1dnextalbumtitle : feel for you / rollercoaster of life . song cocept : life , #yolo , becoming famous ? <3 14 #followmeplz ! <3 x15\\n',\n",
       "       \"workin hard or hardly workin rt <user> at hardee's with my future coworker <user>\\n\",\n",
       "       \"<user> i saw . i'll be replying in a bit .\\n\",\n",
       "       'this is were i belong\\n'],\n",
       "      dtype='<U402')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_three (idx , line ) :\n",
    "    res_set= []\n",
    "    res = []\n",
    "    for word in line.split(' ') :\n",
    "        change_nb = vocab.get(word)\n",
    "        if change_nb != None :\n",
    "            res_set += [change_nb]\n",
    "    for change_nb in set(res_set) :\n",
    "        res += [[1 , idx , change_nb]]\n",
    "    return res\n",
    "\n",
    "def to_three_no_res (idx , line ) :\n",
    "    res = []\n",
    "    for word in line.split(' ') :\n",
    "        change_nb = vocab.get(word)\n",
    "        if change_nb != None :\n",
    "            res += [[1 , idx , change_nb]]\n",
    "        \n",
    "    return res\n",
    "#tweets_to_sparse = [ np.array(elem) for idx , line in enumerate(tweets,0) for elem in to_three(idx , line)]\n",
    "\n",
    "def columns ( matrix , i) :\n",
    "    return [x[i] for x in matrix]\n",
    "\n",
    "\n",
    "def tweet_to_matrix ( tweets ) :\n",
    "    tweets_to_sparse = [ np.array(elem) for idx , line in enumerate(tweets,0) for elem in to_three_no_res(idx , line)]\n",
    "    len(tweets)\n",
    "    len(glove)\n",
    "    sparse_tweets = coo_matrix((columns(tweets_to_sparse,0) , (columns(tweets_to_sparse,1),columns(tweets_to_sparse,2))) , shape=(len(tweets), len(glove)))\n",
    "    print(sparse_tweets.shape)\n",
    "    return sparse_tweets.dot(glove)/len(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2500000, 21161)\n"
     ]
    }
   ],
   "source": [
    "#sparse_tweets = coo_matrix((columns(0) , (columns(1),columns(2))) , shape=(len(tweets), len(glove)))\n",
    "#tweets_res = sparse_tweets.dot(glove)\n",
    "tweets_res = tweet_to_matrix(tweets_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(909, 2500000, 0.0003636, 2500000)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "count2 = 0\n",
    "for x in tweets_res :\n",
    "    count2 +=1\n",
    "    if  np.count_nonzero(x) == 0 :\n",
    "        count +=1\n",
    "count , len(tweets_res) , count/len(tweets_res) , count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w,loss = imp.least_squares(np.array(smile), tweets_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  31514.63958452,  -27901.15107169,    9906.7080602 ,\n",
       "       -142201.4842069 ,   22315.45455723,  -50940.86762591,\n",
       "         34180.25383121,    9050.40040402,   -3038.93689487,\n",
       "        -31158.54848541,   25514.65142868,  -47528.16036134,\n",
       "        -51290.97483126,  -21610.69361479,  -24134.04787436,\n",
       "         18068.75198584,  -42610.54816257,  -22213.88553308,\n",
       "         41445.03505778,  -47413.61386134])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 21161)\n"
     ]
    }
   ],
   "source": [
    "ids_test=[]\n",
    "tweets_test=[]\n",
    "with open('test_data.txt') as f :\n",
    "    ids , tweets_test = split_id_tweet(f.readlines())\n",
    "    \n",
    "tweets_res_test = tweet_to_matrix(tweets_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sea doo pro sea scooter ( sports with the portable sea-doo seascootersave air , stay longer in the water and ... <url>\\n',\n",
       " \"<user> shucks well i work all week so now i can't come cheer you on ! oh and put those batteries in your calculator ! ! !\\n\",\n",
       " 'i cant stay away from bug thats my baby\\n',\n",
       " \"<user> no ma'am ! ! ! lol im perfectly fine and not contagious anymore lmao\\n\",\n",
       " 'whenever i fall asleep watching the tv , i always wake up with a headache\\n',\n",
       " \"<user> he needs to get rid of that thing ! it scares me lol but he don't need a car either . he needs drivers ed again .\\n\",\n",
       " 'its whatever . in a terrible mood ( (\\n',\n",
       " \"yesss ! rt <user> <user> thanks jordan , i love you and i'm gonna call you later !\\n\",\n",
       " 'my friend <user> text me to check up on me last night .\\n',\n",
       " '<user> #followback please . when will ur #unitytour come to europe and sweden ? ?\\n']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_test[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10000, 0.0008, 10000)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "count2 = 0\n",
    "for x in tweets_res_test :\n",
    "    count2 +=1\n",
    "    if  np.count_nonzero(x) == 0 :\n",
    "        count +=1\n",
    "count , len(tweets_res_test) , count/len(tweets_res_test) , count2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "res = tweets_res_test.dot(w)\n",
    "res = np.sign(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp.create_csv_submission(ids,res,'results_vanilla.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8, 10000, 0.0008)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "for x in res :\n",
    "    if x  == 0 :\n",
    "        count +=1\n",
    "count , len(res) , count/len(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
